# -*- coding: utf-8 -*-
"""Merc-Greener-Manufacturing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cu5bMFXY3_LFv4iEhGIiD18OPu33lND1

# Loading the datasets
"""

import pandas as pd

# Loading the datasets
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

# Displaying the first few rows of the train dataset
print(train_df.head())

"""# Data Preprocessing
Removing Low Variance Features
"""

from sklearn.feature_selection import VarianceThreshold

# Assuming all columns except the target are features
features = train_df.columns[train_df.columns != 'target']  # Adjust 'target' if your label column is named differently

# Select only numerical columns
numerical_features = train_df[features].select_dtypes(include=['int64', 'float64'])

# Initialize VarianceThreshold
selector = VarianceThreshold(threshold=0)

# Fit the model to the numerical features of the training data
selector.fit(numerical_features)

# Transform training and testing datasets
train_reduced = train_df[train_df.columns[selector.get_support(indices=True)]]
test_reduced = test_df[test_df.columns[selector.get_support(indices=True)]]

print("Columns retained:", train_reduced.columns)
print("Original number of features:", len(numerical_features.columns))
print("Reduced number of features:", len(train_reduced.columns))

train_reduced.to_csv('train_cleaned.csv', index=False)
test_reduced.to_csv('test_cleaned.csv', index=False)

"""Checking for null values"""

#checking for null values in the training set
null_values_train = train_df.isnull().sum()
print("Null values in the training set: \n", null_values_train)

#checking for null values in the test set
null_values_test = test_df.isnull().sum()
print("Null values in the test set: \n", null_values_test)

"""Applying label encoder"""

from sklearn.preprocessing import LabelEncoder

#storing the encoder for eavh column
encoders = {}

for col in categorical_cols:
  #creating a label encoder for each categorical column
  encoder = LabelEncoder()

  #combining the data from train and test datasets to fit the encoder
  combined_data = pd.concat([train_df[col], test_df[col]], axis = 0).dropna().astype(str)

  #fiting the encoder on the combined data
  encoder.fit(combined_data)

  #storing the fitted encoder in the dictionary
  encoders[col] = encoder

  #transforming the testing and training data using the fitted encoder
  train_df[col] = encoder.transform(train_df[col].astype(str))
  test_df[col] = encoder.transform(test_df[col].astype(str))

print(train_df.head())
print(test_df.head())

"""Dimensionality Reduction"""

from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import pandas as pd

# Excluding the target variable 'y' from the feature set before scaling
features = train_df.columns.drop('y')  # Make sure to adjust this if your target variable has a different name

# Fit the scaler on the training data without the target variable
scaler = StandardScaler()
train_scaled = scaler.fit_transform(train_df[features])

# Apply the same transformation to the test data
test_scaled = scaler.transform(test_df[features])

# Applying PCA (principal Component Analysis)
pca = PCA()
train_pca = pca.fit_transform(train_scaled)

test_pca = pca.transform(test_scaled)

print ("Shape of PCA - transformed training data:", train_pca.shape)
print("Shape of PCA- transformed testing data:", test_pca.shape)

"""XGBoost"""

import xgboost as xgb

# Splitting the features and target variable in training data
X_train = train_df.drop('y', axis=1)  # Adjust this if your features are stored differently
y_train = train_df['y']

# Assuming test_df is already prepared similar to X_train
X_test = test_df

#creating a DMatrix
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test)

#setting XGBoost Parameters

params = {
    'max_depth': 3,
    'eta': 0.1,
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse'
}
num_boost_round = 100

#training the model
bst = xgb.train(params, dtrain, num_boost_round)

#making predictions
y_pred = bst.predict(dtest)
print(y_pred)